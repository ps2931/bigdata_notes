---------------------------
UST Global
---------------------------

Q. Reverse a string
----------------
def reverse(s: String) : String =
    (for(i <- s.length - 1 to 0 by -1) yield s(i)).mkString

Q. Write a program
------------------

Input:
val removeChars =  Set('%', '-', '&', ':')
val words = List("XYZ","12%", "MALE", "ABC","1-7KG", "12KG", "MALE&FEMELE", "1:1","script")

// Output: ['MALE','ABC','XYZ','script']

scala> words.filterNot(w => removeChars.exists(x => w.contains(x)))
res0: List[String] = List(XYZ, MALE, ABC, 12KG, script)

Q. map vs flatMap? Which one is faster?

Q. grouByKey vs reduceByKey?

Q. What is DAG, TASK, Catalyst Optimizer?

Q. Write code to find 4th largest salary
val windowSpec = Window.partitionBy("salary").orderBy(desc("salary"))
val df2 = empDF.withColumn("rank", row_number().over(windowSpec))
df2.filter("rank == 4")


Q. Scenario
-----------
4 Node
64GB RAM * 4
64 cores per node
Data is 200GB = 204800 MB / 128MB = 1600 partitions

1600/4 = 400 partitions per node (400 * 128)=> 51200MB per node will process

65536MB RAM per node 

20 executors per node => each executor will process 51200/20 = 2560MB of data.

5 cores per executor.
Total 20 executors. Therefore total workers 20 * 5 = 100

20 executor on 4 nodes. That means 5 executors per node.

(200*1024)/100

Q. How to delete unwanted files from a directory but keep all text files.
find . -maxdepth 1 -type f -name '[^part-0*.txt]' -delete

shopt -s extglob
rm !(part-0*.txt)

Q. What happens when an Kafka ISR lag happens?
The leader will remove the out-of-sync replica, so as to prevent the message write latency from increasing. 
As long as the out-of-sync replica is still alive, it keeps pulling message from the leader. 
Once the out-of-sync replica catches up the leader’s log end offset, it will be added back to the ISR.

Q. How to resolve merge conflict?
https://opensource.com/article/20/4/git-merge-conflict

Open conflict file. Delete any unwanted content along with conflict signs. Save the file. Then git add & git commit.

Q. What are disadvantages of waterfall model?
Changes are difficult to accomodate.
No working software is produced until late during the life cycle.

Q. How work is assigned in your team?


---------------------------
CGI
---------------------------

Coding Spark program => word count

SQL Analytical functions (rank, dense rank, window function)
    Find 2nd largest salary

SQL Queries
Hive Optimization (Partitioning, Bucketing, Join Optimization)

Spark Optimization (Catalyst Optimizer, Join, spark-submit)
Jobs, lineage, DAG, Stage, Task,
groupByKey vs reduceByKey

Project Discussion

// word count
val textDatDf = spark.read.format("text").load("words.txt")
textDatDf
    .map(line => line.getString(0).split(" "))
    .select(explode($"value").alias("word"))
    .groupBy("word").count.orderBy(desc("count"))
    .show

// or
val textFileRdd = spark.sparkContext.textFile("words.txt")
val counts = textFileRdd
                .flatMap(line => line.split(" "))
                .map(word => (word, 1))
                .reduceByKey(_ + _)

List of string are there and u have to find the list of Arrays containing the strings consist of same character
Input: abc cba azf bca fza
Output: [abc bca cba] [azf fza]

-----------------------
Concentrix
-----------------------
They major focus on SQL part
And optimizations in hive and spark

------------------------
Societe Generale
------------------------

Scala:
What are case classes?
What are closure?

// Find is words are in aString
val words = Array("cat","bat","hat")
val aString = "abcdadt"

// cat and bat are in aString

var flag = true
for (word <- words) {    
    val arr = word.toCharArray
    for (i <- 0 until arr.size) {
        if(!str.contains(arr(i))) {
            flag = false
        }
    }
    if(flag) {
        println(word + " found in " + str)
    }
}


Prob:
Find max profit (purchase - sale) which we can get out of the array.
Once you purchased a stock, you can sale it on back date. That is
if you purchased stock(4) you can not sale it at stock(3)

val stock = Array(50,12,2,3,15,20,1,5)

var max = 0
var temp = 0
for (i <- 0 until stock.size) {    
    for(j <- i + 1 until stock.size) {
        max = stock(j) - stock(i)
        if(max > temp) {
            temp = max            
            println(s"$i, $j, max: $max")
        }
    }
}



// Prob: Read multiheader (same header) file
id,name
id,name
1,abc
2,def
3,ghi

As you can see there are two lines of header

val df = spark.read.option("header", "true").csv("multiheader.csv")
df.show()
+---+----+
| id|name|
+---+----+
| id|name|
|  1| abc|
|  2| def|
|  3| ghi|
+---+----+

val firstRow = df.first()
val df2 = df.filter(row => row != firstRow)
df2.show()
+---+----+
| id|name|
+---+----+
|  1| abc|
|  2| def|
|  3| ghi|
+---+----+


// Read multiheader (different headers) file
id,name
emp_id,emp_name
1,aaa
2,bbb
3,ccc

// Prob: Drop the first header but keep the second header.

val df1 = spark.read.csv("multiheader.csv")
val unwantedRow = df1.first
val df2 = df1.filter(row => row != unwantedRow)

val wantedHeader = df2.first
val df3 = df2.filter(row => row != wantedHeader)

val colArr = df3.columns

val df4 = df3
    .withColumnRenamed(colArr(0), headerRow.getString(0))
    .withColumnRenamed(colArr(1), headerRow.getString(1))


// Second way
val df = spark.read
    .option("header", "true")
    .csv("multiheader.csv")

val unwantedHeader = df.columns
val wantedHeader = df.first

val df2 = df
    .withColumnRenamed(unwantedHeader(0), (wantedHeader.getString(0)))
    .withColumnRenamed(unwantedHeader(1), wantedHeader.getString(1))
    .filter(row => row != wantedHeader)


// Prob: Drop the second header but keep the first header.
id,name
emp_id,emp_name
1,aaa
2,bbb
3,ccc

val df = spark.read.option("header","true").csv("multiheader.csv")
val unwantedHeaderRow = df.first
val df2 = df.filter(row => row != unwantedHeaderRow)

Different types of Hive Metastore?
    - Embedded Metastore
    - Local Metastore
    - Remote Metastore

Embedded Metastore:
In Hive by default, metastore service runs in the same JVM as the Hive service. It uses embedded derby database stored on the local file system in this mode.

Problem:
Only one embedded Derby database can access the database files on disk at any one time, so only one Hive session could
be open at a time.

Local Metastore:
Allow multiple Hive sessions.

Any standalone database (MySQL or PostgreSQL or any other db with JDBC driver) can be used as a metastore.

Hive & Metastore services will run inside the same JVM. RDBMS itself can run in another JVM on a different machine.

Same JVM Process
 -----------------------
| Drive ----> Metastore | ----> MySQL (Separate JVM Process/or Machine)
 -----------------------

This configuration is called as local metastore because metastore service still runs in the same process as the Hive. But it connects to a database running in a separate process, either on the same machine or on a remote machine.

Remote Metastore:
In this mode, metastore runs on its own separate JVM, not in the Hive service JVM. If other processes want to communicate with the metastore server they can communicate using Thrift Network APIs.

We can also have one more metastore servers in this case to provide more availability. This also brings better manageability/security because the database tier can be completely firewalled off.


HashMap vs HashTable?
- HashMap is non-synchronized. It is not thread-safe and can’t be shared between many threads without proper synchronization code whereas Hashtable is synchronized. It is thread-safe and can be shared with many threads.
- HashMap allows one null key and multiple null values whereas Hashtable doesn’t allow any null key or value.
- HashMap is generally preferred over HashTable if thread synchronization is not needed

How to synchronized an arraylist in java?

1. Collections.synchronizedList() method.

synchronized(list)
{
   // must be in synchronized block
   Iterator it = list.iterator();

   while (it.hasNext())
       System.out.println(it.next());
}

2. Using CopyOnWriteArrayList.

// creating a thread-safe Arraylist.
public void usingCopyOnWrite() {
    CopyOnWriteArrayList<String> threadSafeList
        = new CopyOnWriteArrayList<String>();

    // Adding elements to synchronized ArrayList
    threadSafeList.add("geek");
    threadSafeList.add("code");
    threadSafeList.add("practice");

    System.out.println("Elements of synchronized ArrayList :");

    // Iterating on the synchronized ArrayList using iterator.
    Iterator<String> it = threadSafeList.iterator();

    while (it.hasNext())
        System.out.println(it.next());
}

It throws UnsupportedOperationException if you try to modify CopyOnWriteArrayList through iterator’s own method(e.g. add(), set(), remove()).

final vs finally vs finalize in Java?

What is closure?

What is currying?
Currying is the process of converting a function with multiple arguments into a sequence of functions that take one argument. Each function returns another function that consumes the following argument.

What is partial functions?

val positive: PartialFunction[Int, Int] = {
  case x if x >= 0 => x
}

val odd: PartialFunction[Int, Boolean] = {
  case x if x % 2 == 1 => true
}

val even: PartialFunction[Int, Boolean] = {
  case x if x % 2 == 0 => true
}

val evenCheck: PartialFunction[Int, Boolean] = positive andThen even
val oddCheck: PartialFunction[Int, Boolean] = positive andThen odd

evenCheck.isDefinedAt(-2)

What are sealed classes?
A final modifier in Scala makes a class or trait unavailable for an extension. Therefore, it’s quite restrictive. On the other hand, making a class public allows any other class to extend from it.

What if we want something in between these two?

The sealed keyword is used to control the extension of classes and traits. Declaring a class or trait as sealed restricts where we can define its subclasses — we have to define them in the same source file.
Using sealed classes, we can guarantee that only subclasses defined in the file exist. This helps the compiler know all the subclasses of the sealed class. Therefore, this behavior is useful in scenarios like pattern matching.

--------------------------
Luxoft 2nd Round Technical
--------------------------

Q. What are traits?
Q. What are mixin traits?
Q. How to load data from Sqoop to DB if there is no primary key in the table?
Either use -m 1 (or) --split-by <RDBMS-Column>

Q.
Input:
Empid empname transaction created_at 
12      A       D           1-06
12      A       C           2-06
13      B       C           1-06
13      B       C           2-06
14      C       D           1-06
14      C       D           2-06
14      C       D           3-06

Output:
12 A D 1-06
12 A C 2-06
13 B C 2-06
14 C D 3-06

val df = spark
    .read.format("csv")
    .option("header", "true")
    .load("empdata.csv")

val windowSpec = Window
    .partitionBy("empid", "empname", "transaction")
    .orderBy(desc("created_at"))

df
  .withColumn("rnum", row_number().over(windowSpec))
  .select("*")
  .where("rnum == 1")
  .show(false)


Q.
There are 6 Nodes, 32 Cores in total and 128 GB RAM in total. Data size is 1TB.
Find number of cores per executors.

Available cores = 32 - 6 = 26
Available RAM = 128 - 6 = 122

Cores per node = 26/6 => 4 (approx) 
  Either 2 cores per executors 
  or 1 core per executor

If 2 cores per executors
  RAM per node = 122
  RAM per executor = 122/6 => 20 (approx)/2 => 10 GB per executors
If 1 core per executor 
  RAM per node = 122
  RAM per executor = 122/6 => 20 (approx)/4 => 5 GB per executor

------------------------
Coforge
------------------------

Mask email id

val maskEmail =  (email:String) => {
  val dotIndex = email.indexOf('.')
  val domainIndex = email.indexOf('@')

  val fname = email.substring(0, dotIndex)
  val domain = email.substring(domainIndex)

  val maskedEmail = fname + "." + "xxx" + domain
  maskedEmail
}
val maskEmailUDF = udf(maskEmail)

val df = sc.parallelize(Seq(
    ("abc.xyz@gmail.com"), 
    ("aaa.xyz@gmail.com")
  )).toDF("email")

df.withColumn("maskEmail", maskEmailUDF($"email")).show

----------------------
Wallmart
----------------------

Larget sum of 3 elements in an array.
Input: [126,125,67,28, 658] Output: [9, 8, 13, 10, 19]

def sumOfDigits(n: Int): Int = {
  var sum = 0
  var number = n;
  while(number > 0) {
    sum += number % 10
    number = number / 10
  }
  sum
}

scala> numbers
res0: List[Int] = List(126, 125, 67, 28, 658)

scala> numbers.map(n => sumOfDigits(n))
res1: List[Int] = List(9, 8, 13, 10, 19)

CDC (Capture Data Change) in Spark/Hive.
Generate an 8 digit password.
Merging two arrays without using a third array.
SQL Queries (running total)
