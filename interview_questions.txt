---------------------------
UST Global
---------------------------

Q. Reverse a string
----------------
def reverse(s: String) : String =
    (for(i <- s.length - 1 to 0 by -1) yield s(i)).mkString

Q. Write a program
------------------
val removeChars =  List('%', '-', '&', ':')
val words = ('XYZ','12%', 'MALE', 'ABC','1-7KG', '12KG', 'MALE&FEMELE', '1:1',"script")

// Output: ['MALE','ABC','XYZ','script']

scala> words.filterNot(w => removeChars.toSet.exists(x => w.contains(x)))
res0: List[String] = List(XYZ, MALE, ABC, 12KG, script)

Q. map vs flatMap? Which one is faster?

Q. grouByKey vs reduceByKey?

Q. What is DAG, TASK, Catalyst Optimizer?

Q. Write code to find 4th largest salary
val windowSpec = Window.partitionBy("salary").orderBy(desc("salary"))
val df2 = empDF.withColumn("rank", row_number().over(windowSpec))
df2.filter("rank == 4")


Q. Scenario
-----------
4 Node
64GB RAM * 4
64 cores per node
Data is 200GB = 204800 MB / 128MB = 1600 partitions

1600/4 = 400 partitions per node (400 * 128)=> 51200MB per node will process

65536MB RAM per node 

20 executors per node => each executor will process 51200/20 = 2560MB of data.

5 cores per executor.
Total 20 executors. Therefore total workers 20 * 5 = 100

20 executor on 4 nodes. That means 5 executors per node.

(200*1024)/100

Q. How to delete unwanted files from a directory but keep all text files.
find . -maxdepth 1 -type f -name '[^part-0*.txt]' -delete

shopt -s extglob
rm !(part-0*.txt)

Q. What happens when an Kafka ISR lag happens?
The leader will remove the out-of-sync replica, so as to prevent the message write latency from increasing. 
As long as the out-of-sync replica is still alive, it keeps pulling message from the leader. 
Once the out-of-sync replica catches up the leaderâ€™s log end offset, it will be added back to the ISR.

Q. How to resolve merge conflict?
https://opensource.com/article/20/4/git-merge-conflict

Open conflict file. Delete any unwanted content along with conflict signs. Save the file. Then git add & git commit.

Q. What are disadvantages of waterfall model?
Changes are difficult to accomodate.
No working software is produced until late during the life cycle.

Q. How work is assigned in your team?


---------------------------
CGI
---------------------------

Coding Spark program => word count

SQL Analytical functions (rank, dense rank, window function)
    Find 2nd largest salary

SQL Queries
Hive Optimization (Partitioning, Bucketing, Join Optimization)

Spark Optimization (Catalyst Optimizer, Join, spark-submit)
Jobs, lineage, DAG, Stage, Task,
groupByKey vs reduceByKey

Project Discussion

// word count
val textDatDf = spark.read.format("text").load("words.txt")
textDatDf
    .map(line => line.getString(0).split(" "))
    .select(explode($"value").alias("word"))
    .groupBy("word").count.orderBy(desc("count"))
    .show

// or
val textFileRdd = spark.sparkContext.textFile("words.txt")
val counts = textFileRdd
                .flatMap(line => line.split(" "))
                .map(word => (word, 1))
                .reduceByKey(_ + _)

List of string are there and u have to find the list of Arrays containing the strings consist of same character
Input: abc cba azf bca fza
Output: [abc bca cba] [azf fza]

-----------------------
Concentrix
-----------------------
They major focus on SQL part
And optimizations in hive and spark

------------------------
Societe Generale
------------------------

Scala:
What are case classes?
What are closure?

// Find is words are in aString
val words = Array("cat","bat","hat")
val aString = "abcdadt"

// cat and bat are in aString

var flag = true
for (word <- words) {    
    val arr = word.toCharArray
    for (i <- 0 until arr.size) {
        if(!str.contains(arr(i))) {
            flag = false
        }
    }
    if(flag) {
        println(word + " found in " + str)
    }
}


Prob:
Find max profit (purchase - sale) which we can get out of the array.
Once you purchased a stock, you can sale it on back date. That is
if you purchased stock(4) you can not sale it at stock(3)

val stock = Array(50,12,2,3,15,20,1,5)

var max = 0
var temp = 0
for (i <- 0 until stock.size) {    
    for(j <- i + 1 until stock.size) {
        max = stock(j) - stock(i)
        if(max > temp) {
            temp = max            
            println(s"$i, $j, max: $max")
        }
    }
}



// Prob: Read multiheader (same header) file
id,name
id,name
1,abc
2,def
3,ghi

As you can see there are two lines of header

val df = spark.read.option("header", "true").csv("multiheader.csv")
df.show()
+---+----+
| id|name|
+---+----+
| id|name|
|  1| abc|
|  2| def|
|  3| ghi|
+---+----+

val firstRow = df.first()
val df2 = df.filter(row => row != firstRow)
df2.show()
+---+----+
| id|name|
+---+----+
|  1| abc|
|  2| def|
|  3| ghi|
+---+----+


// Read multiheader (different headers) file
id,name
emp_id,emp_name
1,aaa
2,bbb
3,ccc

// Prob: Drop the first header but keep the second header.

val df1 = spark.read.csv("multiheader.csv")
val unwantedRow = df1.first
val df2 = df1.filter(row => row != unwantedRow)

val wantedHeader = df2.first
val df3 = df2.filter(row => row != wantedHeader)

val colArr = df3.columns

val df4 = df3
    .withColumnRenamed(colArr(0), headerRow.getString(0))
    .withColumnRenamed(colArr(1), headerRow.getString(1))


// Second way
val df = spark.read
    .option("header", "true")
    .csv("multiheader.csv")

val unwantedHeader = df.columns
val wantedHeader = df.first

val df2 = df
    .withColumnRenamed(unwantedHeader(0), (wantedHeader.getString(0)))
    .withColumnRenamed(unwantedHeader(1), wantedHeader.getString(1))
    .filter(row => row != wantedHeader)


// Prob: Drop the second header but keep the first header.
id,name
emp_id,emp_name
1,aaa
2,bbb
3,ccc

val df = spark.read.option("header","true").csv("multiheader.csv")
val unwantedHeaderRow = df.first
val df2 = df.filter(row => row != unwantedHeaderRow)

